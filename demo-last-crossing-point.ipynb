{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as trans\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math, random\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import waveform_tooling as wt\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mkl.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input file into a set of waveforms\n",
    "\n",
    "inputFile = \"./LJ001-0005-44100.wav\"\n",
    "outputPath = \"./waveforms/human-voice/wav\"\n",
    "\n",
    "y, sr = librosa.load(inputFile, sr = None)\n",
    "\n",
    "y = y / np.amax(np.absolute(y))\n",
    "\n",
    "wmap = wt.getWavelengthMap(y, 66)\n",
    "waveforms, wavelengths = wt.split(y, wmap, oversampling = 2, encoderWidth = 600, mode = wt.lastCrossingPoint)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "os.makedirs(outputPath, exist_ok = True)\n",
    "for waveform in waveforms:\n",
    "    sf.write(outputPath + f'/%04d.wav' % counter, waveform, 44100, 'PCM_24')\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(tuple([x.size(0)] + list(self.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "\n",
    "n_latent_dim = 32\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        n_channels = 256\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, n_channels, 25, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(n_channels, n_channels, 25, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(n_channels, n_channels, 5, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(n_channels, n_channels, 5, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(n_channels, n_channels, 3, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool1d(2),\n",
    "            Flatten(),\n",
    "            nn.Linear(n_channels * 14, n_channels),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_channels, n_latent_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent_dim, n_channels),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_channels, n_channels * 14),\n",
    "            nn.Tanh(),\n",
    "            Reshape(n_channels, 14),\n",
    "            nn.Upsample(scale_factor = 2, mode='nearest'),\n",
    "            nn.ConvTranspose1d(n_channels, n_channels, 3, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor = 2, mode='nearest'),\n",
    "            nn.ConvTranspose1d(n_channels, n_channels, 5, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor = 2, mode='nearest'),\n",
    "            nn.ConvTranspose1d(n_channels, n_channels, 5, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor = 2, mode='nearest'),\n",
    "            nn.ConvTranspose1d(n_channels, n_channels, 25, 1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor = 2, mode='nearest'),\n",
    "            nn.ConvTranspose1d(n_channels, 1, 25, 1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "def wav_loader(path):\n",
    "    (y, sr) = librosa.load(path, sr = None)\n",
    "\n",
    "    return torch.tensor(y)\n",
    "\n",
    "def npy_loader(path):\n",
    "    sample = torch.from_numpy(np.load(path))\n",
    "    return sample\n",
    "\n",
    "def load(encoder, decoder, map_location):\n",
    "    autoencoder = Autoencoder().cuda()\n",
    "    autoencoder.encoder = torch.load(encoder, map_location=map_location)\n",
    "    autoencoder.decoder = torch.load(decoder, map_location=map_location)\n",
    "    return autoencoder\n",
    "\n",
    "def train(datasetFolder):\n",
    "    batchSize = 500\n",
    "    \n",
    "    data = ds.DatasetFolder(datasetFolder, \n",
    "        loader=wav_loader,\n",
    "        extensions='.wav')\n",
    "\n",
    "    sampleloader = torch.utils.data.DataLoader(data, batch_size=batchSize,\n",
    "                                              shuffle=True, num_workers=0)\n",
    "\n",
    "    sample_loss_criterion = nn.L1Loss().cuda()\n",
    "    spectral_loss_criterion = nn.L1Loss().cuda()\n",
    "    autoencoder_optimizer = optim.Adam(autoencoder.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(10000):\n",
    "        running_spectral_loss = 0.0\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        if epoch == 50:\n",
    "            autoencoder_optimizer = optim.Adam(autoencoder.parameters(), lr=0.00002, betas=(0.5, 0.999))\n",
    "\n",
    "        for i, inputs in enumerate(sampleloader, 0):\n",
    "            if inputs[0].size()[0] < batchSize:\n",
    "                continue\n",
    "                \n",
    "            input = inputs[0].unsqueeze(1).cuda()\n",
    "\n",
    "            autoencoder_optimizer.zero_grad()\n",
    "            encoded = autoencoder(input)\n",
    "\n",
    "            e_spec = torch.rfft(encoded.squeeze(1), 1)\n",
    "            i_spec = torch.rfft(input.squeeze(1), 1)\n",
    "            \n",
    "            spectral_loss = spectral_loss_criterion(e_spec, i_spec)\n",
    "\n",
    "            spectral_loss.backward()\n",
    "            autoencoder_optimizer.step()\n",
    "\n",
    "            running_spectral_loss += spectral_loss.item()\n",
    "            counter += 1\n",
    "\n",
    "        print('[%s] [%d] loss: %.5f' %\n",
    "          (datetime.datetime.now(), \n",
    "            epoch + 1, \n",
    "            running_spectral_loss / counter))\n",
    "        \n",
    "        if(epoch % 10 == 0 and epoch > 0):\n",
    "            testInput = wav_loader('./waveforms/human-voice-test/1570.wav').unsqueeze(0).unsqueeze(0).cuda()\n",
    "\n",
    "            testEncoded = autoencoder(testInput)\n",
    "\n",
    "            plt.figure(1)\n",
    "            plt.title(\"Original\")\n",
    "            plt.plot(testInput.squeeze(0).squeeze(0).cpu().detach().numpy())\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(2)\n",
    "            plt.title(\"Encoded\")\n",
    "            plt.plot(testEncoded.squeeze(0).squeeze(0).cpu().detach().numpy())\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "\n",
    "#autoencoder = Autoencoder().cuda()\n",
    "autoencoder = load(\"./human-voice-last-crossing-point-encoder.net\", \"./human-voice-last-crossing-point-decoder.net\", \"cuda\")\n",
    "#train(\"./waveforms/human-voice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process the waveforms\n",
    "\n",
    "inputPath = './waveforms/human-voice/wav/'\n",
    "outputPath = './waveforms/human-voice-output/'\n",
    "\n",
    "os.makedirs(outputPath, exist_ok=True)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "latent_state = torch.zeros(1, 1, n_latent_dim).cuda()\n",
    "\n",
    "for f in os.listdir(inputPath):\n",
    "    input = wav_loader(inputPath + f).unsqueeze(0).unsqueeze(0).cuda()\n",
    "    \n",
    "    latent = autoencoder.encoder(input).detach()\n",
    "    \n",
    "    latent_state = latent_state.lerp(latent, 0.5)\n",
    "    \n",
    "    encoded = autoencoder.decoder(latent_state)\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "    sf.write(outputPath + f, encoded.squeeze(0).squeeze(0).cpu().detach().numpy(), 44100, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back\n",
    "\n",
    "inputPath = './waveforms/human-voice-output/'\n",
    "\n",
    "outputFilename = 'render-human-voice.wav'\n",
    "\n",
    "waveforms = []\n",
    "for f in os.listdir(inputPath):\n",
    "    y, sr = librosa.load(inputPath + f, sr = None)\n",
    "    waveforms.append(y)\n",
    "    \n",
    "output = wt.merge(waveforms, oversampling = 2, mode = wt.lastCrossingPoint)\n",
    "sf.write('./' + outputFilename, output, 44100, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
